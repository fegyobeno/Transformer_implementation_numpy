{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d845af",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "205d84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffab2f",
   "metadata": {},
   "source": [
    "# Implement a numerically stable softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "caca1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\"Compute the softmax of each element along an axis of X.\"\"\"\n",
    "    # Subtract the max for numerical stability\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    sum_e_x = np.sum(e_x, axis=axis, keepdims=True)\n",
    "    return e_x / sum_e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2ad14889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax result:\n",
      " [0.08714432 0.23688282 0.64391426 0.0320586 ]\n",
      "Sum along axis -1 (should be 1):\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "# Testing softmax function\n",
    "x = np.array([1,2,3,0])\n",
    "print(\"Softmax result:\\n\", softmax(x, axis=-1))\n",
    "print(\"Sum along axis -1 (should be 1):\\n\", np.sum(softmax(x, axis=-1), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b285b49",
   "metadata": {},
   "source": [
    "# Implement the scaled dot-prodcut attention mechanism\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3cf738d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute scaled dot-product attention.\n",
    "    Args:\n",
    "        Q: queries shape (batch, seq_len_q, depth)\n",
    "        K: keys shape (batch, seq_len_k, depth)\n",
    "        V: values shape (batch, seq_len_v, depth_v)\n",
    "        mask: optional mask broadcastable to (batch, seq_len_q, seq_len_k) where masked positions are True/1\n",
    "    Returns:\n",
    "        output: shape (batch, seq_len_q, depth_v)\n",
    "        attention_weights: shape (batch, seq_len_q, seq_len_k)\n",
    "    Notes:\n",
    "        This implementation assumes K and V have compatible sequence lengths and uses the last dimension as depth.\n",
    "    \"\"\"\n",
    "    # Compute raw scores: (batch, seq_q, seq_k)\n",
    "    \n",
    "    # Scaling factor\n",
    "    dk = Q.shape[-1] # equivalent to the depth of K\n",
    "    # Swap the last two axes of K same as K.T in the formula\n",
    "    scores = np.matmul(Q, np.swapaxes(K, -1, -2)) / np.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        # set large negative value to the masked positions\n",
    "        scores = np.where(mask, -1e10, scores)\n",
    "    '''\n",
    "    ###\n",
    "    Attention: Expresses the relevance of each key to a given query.\n",
    "    Output = softmax(QK^T / sqrt(dk)) V\n",
    "    ###\n",
    "    '''\n",
    "    # Apply softmax to get attention weights\n",
    "    attn = softmax(scores, axis=-1)\n",
    "    # Compute the final output\n",
    "    output = np.matmul(attn, V)\n",
    "    return output, attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "8637e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " [[[ 0.81362694 -0.28537062]\n",
      "  [-0.085753   -0.53365127]]]\n",
      "Mask:\n",
      " [[[1 1]\n",
      "  [0 0]]]\n",
      "Output shape: (1, 2, 2)\n",
      "[[[0.20485025 0.11794609]\n",
      "  [0.09490977 0.01956671]]]\n",
      "Attention shape: (1, 2, 2)\n",
      "[[[0.5        0.5       ]\n",
      "  [0.41702649 0.58297351]]]\n"
     ]
    }
   ],
   "source": [
    "# Testing scaled dot-product attention\n",
    "batch, seq_q, seq_k, depth = 1, 2, 2, 2\n",
    "Q = np.random.randn(batch, seq_q, depth)\n",
    "K = np.random.randn(batch, seq_k, depth)\n",
    "V = np.random.randn(batch, seq_k, depth)\n",
    "print(\"Q:\\n\", Q)\n",
    "mask = np.random.rand(batch, seq_q, seq_k) > 0.5\n",
    "print(\"Mask:\\n\", mask.astype(int))\n",
    "output, attn = scaled_dot_product_attention(Q, K, V, mask)\n",
    "print(\"Output shape:\", output.shape) \n",
    "print(output)\n",
    "print(\"Attention shape:\", attn.shape) \n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde430c",
   "metadata": {},
   "source": [
    "# Implement positional encoding\n",
    "\n",
    "inject information about the position of the tokens in a sequence <br>\n",
    "Important as:\n",
    "Eve likes Adam is not equal to Adam likes Eve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "36614445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_len: int, d_model: int) -> np.ndarray:\n",
    "    \"\"\"Generate sinusoidal positional encodings as in the original paper.\n",
    "    Returns array of shape (max_len, d_model).\n",
    "    Args:\n",
    "        max_len: maximum length of the sequence\n",
    "        d_model: dimension of the model's embeddings\n",
    "    \"\"\"\n",
    "    # Create a position matrix\n",
    "    position = np.arange(max_len)[:, np.newaxis]  # (max_len, 1)\n",
    "    # Create a constant for numerical stability\n",
    "    # Compute the positional encodings\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * - (np.log(10000.0) / d_model))\n",
    "    # Create an all zero matrix to be filled with encodings\n",
    "    positional_encoding = np.zeros((max_len, d_model))\n",
    "    # Apply sin to the evenly indexed positions\n",
    "    positional_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "    \n",
    "    # Fix the odd dimensionality issue\n",
    "    if d_model % 2 == 1:\n",
    "        no_odd_dims = positional_encoding[:, 1::2].shape[1]\n",
    "        positional_encoding[:, 1::2] = np.cos(position * div_term[:no_odd_dims])\n",
    "    else:\n",
    "        # Apply cos to the others\n",
    "        positional_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "    return positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ce7f1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding shape: (10, 3)\n",
      "[[ 0.          1.          0.        ]\n",
      " [ 0.84147098  0.54030231  0.00215443]\n",
      " [ 0.90929743 -0.41614684  0.00430886]\n",
      " [ 0.14112001 -0.9899925   0.00646326]\n",
      " [-0.7568025  -0.65364362  0.00861763]\n",
      " [-0.95892427  0.28366219  0.01077197]\n",
      " [-0.2794155   0.96017029  0.01292625]\n",
      " [ 0.6569866   0.75390225  0.01508047]\n",
      " [ 0.98935825 -0.14550003  0.01723462]\n",
      " [ 0.41211849 -0.91113026  0.0193887 ]]\n"
     ]
    }
   ],
   "source": [
    "# Test positional encoding\n",
    "max_len = 10\n",
    "d_model = 3\n",
    "pos_encoding = positional_encoding(max_len, d_model)\n",
    "print(\"Positional Encoding shape:\", pos_encoding.shape)\n",
    "print(pos_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8599439",
   "metadata": {},
   "source": [
    "# Multi headed attention\n",
    "\n",
    "Split the input into multiple heads <br>\n",
    "$${MultiHead(Q,K,V)=Concat(head_1,head_2,...,head_h)*W^O}$$\n",
    "where\n",
    "$${head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7e41bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model: int, num_heads: int, seed: int = 42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: model dimensionality\n",
    "            num_heads: number of attention heads\n",
    "            seed: random seed for weight initialization and reproducibility\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"It is required for the model dimension to be divisible by the num. of heads.\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        # Initialise a random state\n",
    "        generator = np.random.RandomState(seed)\n",
    "\n",
    "        # Initialise weights\n",
    "        # inside __init__\n",
    "        std = 1.0 / np.sqrt(d_model)\n",
    "        self.Wq = generator.normal(scale=std, size=(d_model, d_model))\n",
    "        self.Wk = generator.normal(scale=std, size=(d_model, d_model))\n",
    "        self.Wv = generator.normal(scale=std, size=(d_model, d_model))\n",
    "        self.Wo = generator.normal(scale=std, size=(d_model, d_model))\n",
    "        # self.Wq = generator.normal(scale=0.2, size=(d_model, d_model))\n",
    "        # self.Wk = generator.normal(scale=0.2, size=(d_model, d_model))\n",
    "        # self.Wv = generator.normal(scale=0.2, size=(d_model, d_model))\n",
    "        # self.Wo = generator.normal(scale=0.2, size=(d_model, d_model))\n",
    "        \n",
    "    def split_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, depth).\n",
    "        Args:\n",
    "            x: input of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            reshaped x of shape (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        # x.shape: (batch_size, seq_len, d_model)\n",
    "        batch_size = x.shape[0]\n",
    "        sequence_length = x.shape[1]\n",
    "        x = x.reshape(batch_size, sequence_length, self.num_heads, self.depth)\n",
    "        return np.transpose(x, (0, 2, 1, 3))\n",
    "        \n",
    "        \n",
    "    def combine_heads(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Combine the heads to get back to original d_model dimension.\n",
    "        Args:\n",
    "            x: input of shape (batch_size, num_heads, seq_len, depth)\n",
    "            Returns:\n",
    "            reshaped x of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = np.transpose(x, (0, 2, 1, 3))  # (batch_size, seq_len, ...)\n",
    "        batch_size = x.shape[0]\n",
    "        sequence_length = x.shape[1]\n",
    "        return x.reshape(batch_size, sequence_length, self.d_model)\n",
    "    \n",
    "    def __call__(self, Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate the multi headed attention.\n",
    "        Return a tuple of (output, attention_weights)\n",
    "        Args:\n",
    "            Q: queries shape (batch_size, seq_len_q, d_model)\n",
    "            K: keys shape (batch_size, seq_len_k, d_model)\n",
    "            V: values shape (batch_size, seq_len_v, d_model)\n",
    "            mask: optional mask broadcastable to (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        # Linearly project Q, K, and V\n",
    "        q = Q @ self.Wq  # (batch_size, seq_len_q, d_model)\n",
    "        k = K @ self.Wk  \n",
    "        v = V @ self.Wv  \n",
    "        \n",
    "        # Split the heads into multiple ones\n",
    "        qheads = self.split_heads(q)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        kheads = self.split_heads(k)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        vheads = self.split_heads(v)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        # Batched attention\n",
    "        # Way more efficient than a standard for loop over the heads\n",
    "        # batch, num_heads, sequence_q, depth\n",
    "        batch, num_heads, sequence_q, depth = qheads.shape\n",
    "        _q = qheads.reshape(batch * num_heads, sequence_q, depth)\n",
    "        _k = kheads.reshape(batch * num_heads, kheads.shape[2], depth)\n",
    "        _v = vheads.reshape(batch * num_heads, vheads.shape[2], depth)\n",
    "        \n",
    "        # We need to expand the mask for all the heads\n",
    "        if mask is not None:\n",
    "            mask = np.repeat(mask, self.num_heads, axis=0)\n",
    "\n",
    "        output, attention = scaled_dot_product_attention(_q, _k, _v, mask)\n",
    "        \n",
    "        # Reashape everything back for combining heads\n",
    "        output = output.reshape(batch, num_heads, sequence_q, depth)\n",
    "        combined_output = self.combine_heads(output)  # (batch_size, seq_len_q, d_model)\n",
    "        output = combined_output @ self.Wo  # Multiplying with the final weight \n",
    "        \n",
    "        # Reshape attention back to (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        attention = attention.reshape(batch, num_heads, attention.shape[1], attention.shape[2])\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "34aefae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " [[[-0.42421834 -0.85574172]\n",
      "  [-0.29170982 -0.35202485]]]\n",
      "Mask:\n",
      " [[[1 1]\n",
      "  [0 1]]]\n",
      "Output:\n",
      " [[[ 0.47520055 -0.46817396]\n",
      "  [ 0.35769828 -0.29930607]]]\n",
      "Attention:\n",
      " [[[[0.5 0.5]\n",
      "   [1.  0. ]]\n",
      "\n",
      "  [[0.5 0.5]\n",
      "   [1.  0. ]]]]\n"
     ]
    }
   ],
   "source": [
    "#Test multi headed attention\n",
    "batch, seq_q, seq_k, depth = 1, 2, 2, 2\n",
    "Q = np.random.randn(batch, seq_q, depth)\n",
    "K = np.random.randn(batch, seq_k, depth)\n",
    "V = np.random.randn(batch, seq_k, depth)\n",
    "print(\"Q:\\n\", Q)\n",
    "mask = np.random.rand(batch, seq_q, seq_k) > 0.5\n",
    "print(\"Mask:\\n\", mask.astype(int))\n",
    "\n",
    "mha = MultiHeadAttention(d_model = depth, num_heads = 2, seed = 42)\n",
    "output, attention = mha(Q, K, V, mask)\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Attention:\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae3bb7",
   "metadata": {},
   "source": [
    "# Position wise Feed Forward Network\n",
    "\n",
    "$$FFN(x) = \\text{Linear}_2(\\text{ReLU}(\\text{Linear}_1(x))) $$\n",
    "$$FFN(x) = \\max(0, xW\\_1 + b\\_1)W\\_2 + b\\_2 $$\n",
    "Based on the original paper: <br>\n",
    "\"This consists of two linear transformations with a ReLU activation in between.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "5179ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward:\n",
    "    def __init__(self, d_model: int, d_ff: int, seed: int = 42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: model dimensionality\n",
    "            d_ff: dimensionality of the feed forward network\n",
    "            seed: random seed for weight initialization and reproducibility\n",
    "        \"\"\"\n",
    "        # Random state generator\n",
    "        generator = np.random.RandomState(seed)\n",
    "        self.W1 = generator.normal(scale=0.2, size=(d_model, d_ff))\n",
    "        self.b1 = np.zeros((d_ff,))\n",
    "        self.W2 = generator.normal(scale=0.2, size=(d_ff, d_model))\n",
    "        self.b2 = np.zeros((d_model,))\n",
    "        \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, seq_length, d_model)\n",
    "\n",
    "        Returns:\n",
    "            output tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Apply the first linear transformation\n",
    "        x = x @ self.W1 + self.b1\n",
    "        x = np.maximum(0, x)  # ReLU\n",
    "        x = x @ self.W2 + self.b2\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f3585421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.01458096 -0.08822238 -0.00927982 -0.00983293]\n",
      "  [ 0.00263115 -0.03944536 -0.01325151 -0.01272889]\n",
      "  [ 0.02267821 -0.14046964 -0.02002302 -0.01371408]]]\n"
     ]
    }
   ],
   "source": [
    "# Test Position Wise Feed Forward Network\n",
    "d_model = 4\n",
    "d_ff = 8\n",
    "ffn = PositionWiseFeedForward(d_model, d_ff, seed=42)\n",
    "print(ffn(np.random.rand(1, 3, d_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b345",
   "metadata": {},
   "source": [
    "# Implement the layer normalisation\n",
    "\n",
    "needed for the encoding and decoding stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "927619c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No standard layer normalisation in the original paper, thus this is one possible one. \n",
    "def layer_normalization(x: np.ndarray, epsilon: float = 1e-10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Layer for normalising an input ndarray/tensor.\n",
    "    Args:\n",
    "        x: input tensor of shape (..., features)\n",
    "        epsilon: small float added to variance to avoid dividing by zero\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    var = np.var(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / np.sqrt(var + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "16b389c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Layer Norm:\n",
      " [[[0.97428919 0.74905569 0.74863568 0.8047723 ]\n",
      "  [0.10825874 0.51662637 0.68566105 0.48331562]\n",
      "  [0.80844796 0.34834711 0.46362344 0.93173749]]\n",
      "\n",
      " [[0.46096797 0.75113865 0.8247389  0.35784697]\n",
      "  [0.51525289 0.93192136 0.63555553 0.68139192]\n",
      "  [0.79260925 0.77292006 0.86060247 0.41410598]]]\n",
      "After Layer Norm:\n",
      " [[[ 1.67835292 -0.75890644 -0.76345136 -0.15599512]\n",
      "  [-1.61335633  0.32323837  1.12484859  0.16526937]\n",
      "  [ 0.711211   -1.20904476 -0.72793276  1.22576652]]\n",
      "\n",
      " [[-0.70801805  0.78390927  1.16232872 -1.23821994]\n",
      "  [-1.15843505  1.58755497 -0.36559883 -0.0635211 ]\n",
      "  [ 0.47459353  0.36139692  0.86549868 -1.70148913]]]\n"
     ]
    }
   ],
   "source": [
    "# Test layer normalization\n",
    "x = np.random.rand(2, 3, 4)\n",
    "print(\"Before Layer Norm:\\n\", x)\n",
    "x = layer_normalization(x)\n",
    "print(\"After Layer Norm:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9c3f8",
   "metadata": {},
   "source": [
    "# Encoder layer\n",
    "\n",
    "MHA -> ADD&NORM -> FF -> ADD&NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "48e9f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: model dimensionality\n",
    "            num_heads: number of attention heads\n",
    "            d_ff: dimensionality of the feed forward network\n",
    "            seed: random seed for weight initialization and reproducibility\n",
    "        \"\"\"\n",
    "        self.seed = 42\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, self.seed)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, self.seed)\n",
    "        \n",
    "    def __call__(self, x: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, seq_length, d_model)\n",
    "            mask: optional mask broadcastable to (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        Returns:\n",
    "            output tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        attention_output, _ = self.mha(x, x, x, mask)  # Self-attention\n",
    "        x = x + attention_output  # Residual connection\n",
    "        ann1 = layer_normalization(x)  # Add & Norm\n",
    "        \n",
    "        ffn_output = self.ffn(ann1) # Feed Forward Network\n",
    "        x = ann1 + ffn_output  # Residual connection\n",
    "        output = layer_normalization(x)  # Add & Norm\n",
    "        \n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "89b31774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input with positional encodings:\n",
      " [[[0.80445011 1.32273761 0.71461465 1.90757689]\n",
      "  [1.5240724  1.07455612 0.34228796 1.7038028 ]\n",
      "  [1.17866779 0.45362952 0.57496512 1.95711781]]]\n",
      "Encoder output:\n",
      " [[[ 0.76886472  0.90609297 -1.60725184 -0.06770585]\n",
      "  [ 1.16918237  0.50319328 -1.5366837  -0.13569195]\n",
      "  [ 1.21547546  0.13433746 -1.56753611  0.21772319]]]\n"
     ]
    }
   ],
   "source": [
    "# Test encoder layer\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_ff = 8\n",
    "input_embeddings = np.random.rand(1, 3, 4)\n",
    "positional_encodings = positional_encoding(max_len=3, d_model=d_model)\n",
    "input_with_pos = input_embeddings + positional_encodings[np.newaxis, :3, :]\n",
    "print(\"Input with positional encodings:\\n\", input_with_pos)\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "output = encoder_layer(input_with_pos)\n",
    "print(\"Encoder output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ee9e9",
   "metadata": {},
   "source": [
    "# Decoder layer\n",
    "\n",
    "MMHA -> A&N -> MHA -> A&N -> FF -> A&N -> Lin -> Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "15617676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer:\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: model dimensionality\n",
    "            num_heads: number of attention heads\n",
    "            d_ff: dimensionality of the feed forward network\n",
    "            seed: random seed for weight initialization and reproducibility\n",
    "        \"\"\"\n",
    "        self.seed = 42\n",
    "        self.masked_mha1 = MultiHeadAttention(d_model, num_heads, self.seed)  # Masked MHA\n",
    "        self.encoder_decoder_mha2 = MultiHeadAttention(d_model, num_heads, self.seed)  # MHA with encoder output\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, self.seed)\n",
    "        \n",
    "    def __call__(self, x: np.ndarray, encoder_output: np.ndarray, look_ahead_mask: Optional[np.ndarray] = None, padding_mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, target_seq_length, d_model)\n",
    "            enc_output: encoder output tensor of shape (batch_size, input_seq_length, d_model)\n",
    "            look_ahead_mask: triangular mask to prevent attending future tokens\n",
    "            padding_mask: optional mask for the second MHA\n",
    "        Returns:\n",
    "            output tensor of shape (batch_size, target_seq_length, d_model)\n",
    "        \"\"\"\n",
    "        # Masked Multi-Head Attention\n",
    "        attention_1, _ = self.masked_mha1(x, x, x, look_ahead_mask)  # Self-attention with look-ahead mask\n",
    "        x = x + attention_1  # Residual connection\n",
    "        ann1 = layer_normalization(x)  # Add & Norm\n",
    "        \n",
    "        # Multi-Head Attention with encoder output\n",
    "        attention_2, _ = self.encoder_decoder_mha2(ann1, encoder_output, encoder_output, padding_mask)\n",
    "        x = ann1 + attention_2  # Residual connection\n",
    "        ann2 = layer_normalization(x)  # Add & Norm\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        ffn_output = self.ffn(ann2)\n",
    "        x = ann2 + ffn_output  # Residual connection\n",
    "        output = layer_normalization(x)  # Add & Norm\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "5cea594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output:\n",
      " [[[ 1.02720323  0.87541866 -1.38056066 -0.52206124]\n",
      "  [ 1.37708826  0.48709442 -1.18584163 -0.67834106]\n",
      "  [ 1.54990391 -0.32052063 -1.22271004 -0.00667324]]]\n"
     ]
    }
   ],
   "source": [
    "# Test decoder layer with the previous encoder output\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_ff = 8\n",
    "\n",
    "decoder_layer = DecoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "decoder_input = np.random.rand(1, 3, 4)\n",
    "decoder_pos_enc = positional_encoding(max_len=3, d_model=d_model)\n",
    "# Triangular look-ahead mask\n",
    "look_ahead_mask = np.triu(np.ones((1, 3, 3)), k=1).astype(bool)\n",
    "decoder_output = decoder_layer(decoder_input + decoder_pos_enc, output, look_ahead_mask=look_ahead_mask)\n",
    "\n",
    "print(\"Decoder output:\\n\", decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f6a7f",
   "metadata": {},
   "source": [
    "# Encoder stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fd6921a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_layers: number of encoder layers\n",
    "            d_model: model dimensionality\n",
    "            num_heads: number of attention heads\n",
    "            d_ff: dimensionality of the feed forward network\n",
    "        \"\"\"\n",
    "        self.layers = [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
    "        \n",
    "    def __call__(self, x: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, seq_length, d_model)\n",
    "            mask: optional mask broadcastable to (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        Returns:\n",
    "            output tensor of shape (batch_size, seq_length, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "09b2cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input with positional encodings:\n",
      " [[[0.67408879 1.11600415 0.07145756 1.6237182 ]\n",
      "  [1.630549   0.57626199 0.77138863 1.21269814]\n",
      "  [1.50757462 0.3350594  0.20329144 1.80743073]]]\n",
      "Encoder output:\n",
      " [[[ 1.03662878  0.58078776 -1.60872959 -0.00868695]\n",
      "  [ 1.09657336  0.45023325 -1.60961845  0.06281184]\n",
      "  [ 1.03892233  0.47270247 -1.63748354  0.12585874]]]\n"
     ]
    }
   ],
   "source": [
    "# Test encoder\n",
    "\n",
    "# Test encoder layer\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_ff = 8\n",
    "input_embeddings = np.random.rand(1, 3, 4)\n",
    "positional_encodings = positional_encoding(max_len=3, d_model=d_model)\n",
    "input_with_pos = input_embeddings + positional_encodings[np.newaxis, :3, :]\n",
    "print(\"Input with positional encodings:\\n\", input_with_pos)\n",
    "\n",
    "encoder = Encoder(num_layers=10, d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "output = encoder(input_with_pos)\n",
    "print(\"Encoder output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57497b5e",
   "metadata": {},
   "source": [
    "# Decoder stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b2a6d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_layers: number of decoder layers\n",
    "            d_model: model dimensionality\n",
    "            num_heads: number of attention heads\n",
    "            d_ff: dimensionality of the feed forward network\n",
    "        \"\"\"\n",
    "        self.layers = [DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
    "        \n",
    "    def __call__(self, x: np.ndarray, encoder_output: np.ndarray, look_ahead_mask: Optional[np.ndarray] = None, padding_mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, target_seq_length, d_model)\n",
    "            enc_output: encoder output tensor of shape (batch_size, input_seq_length, d_model)\n",
    "            look_ahead_mask: triangular mask to prevent attending future tokens\n",
    "            padding_mask: optional mask for the second MHA\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, look_ahead_mask, padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "49154ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output:\n",
      " [[[ 1.15950012  0.4638494  -1.56093152 -0.06241799]\n",
      "  [ 1.17375064  0.47676652 -1.54390302 -0.10661414]\n",
      "  [ 1.15429576  0.44697933 -1.57062775 -0.03064734]]]\n"
     ]
    }
   ],
   "source": [
    "# Test decoder with encoder output\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_ff = 8\n",
    "decoder = Decoder(num_layers=10, d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "decoder_input = np.random.rand(1, 3, 4)\n",
    "decoder_output = decoder(decoder_input, output)\n",
    "print(\"Decoder output:\\n\", decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c011bb3",
   "metadata": {},
   "source": [
    "# Full transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "cba60475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int, d_model: int, num_heads: int, d_ff: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_encoder_layers: number of encoder layers\n",
    "            num_decoder_layers: number of decoder layers\n",
    "            d_model: model dimensionality\n",
    "            num_heads: number of attention heads\n",
    "            d_ff: dimensionality of the feed forward network\n",
    "        \"\"\"\n",
    "        self.encoder = Encoder(num_encoder_layers, d_model, num_heads, d_ff)\n",
    "        self.decoder = Decoder(num_decoder_layers, d_model, num_heads, d_ff)\n",
    "\n",
    "    def __call__(self, encoder_input: np.ndarray, decoder_input: np.ndarray, look_ahead_mask: Optional[np.ndarray] = None, padding_mask: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_input: input tensor for the encoder of shape (batch_size, input_seq_length, d_model)\n",
    "            decoder_input: input tensor for the decoder of shape (batch_size, target_seq_length, d_model)\n",
    "            look_ahead_mask: optional mask for the decoder\n",
    "            padding_mask: optional mask for the encoder\n",
    "        \"\"\"\n",
    "        encoder_output = self.encoder(encoder_input, padding_mask)\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, look_ahead_mask, padding_mask)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "53ae3c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer output:\n",
      " [[[-0.48432267  0.71760103 -1.38614123  1.15286287]\n",
      "  [-0.32871729  0.43114645 -1.41151084  1.30908167]]]\n"
     ]
    }
   ],
   "source": [
    "# Test full transformer architecture\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_ff = 8\n",
    "transformer = Transformer(num_encoder_layers=2, num_decoder_layers=2, d_model=d_model, num_heads=num_heads, d_ff=d_ff)\n",
    "encoder_input = np.random.rand(1, 3, 4)\n",
    "decoder_input = np.random.rand(1, 2, 4)\n",
    "output = transformer(encoder_input, decoder_input)\n",
    "print(\"Transformer output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f1681",
   "metadata": {},
   "source": [
    "# Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5814c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_encoder_decoder_forward_shapes (__main__.TestTransformerComponents) ... ok\n",
      "test_ffn_shape (__main__.TestTransformerComponents) ... ok\n",
      "test_ffn_shape (__main__.TestTransformerComponents) ... ok\n",
      "test_multi_head_attention_shapes (__main__.TestTransformerComponents) ... ok\n",
      "test_positional_encoding (__main__.TestTransformerComponents) ... ok\n",
      "test_scaled_dot_product_attention_shape_and_probs (__main__.TestTransformerComponents) ... ok\n",
      "test_transformer_forward_shape (__main__.TestTransformerComponents) ... ok\n",
      "test_multi_head_attention_shapes (__main__.TestTransformerComponents) ... ok\n",
      "test_positional_encoding (__main__.TestTransformerComponents) ... ok\n",
      "test_scaled_dot_product_attention_shape_and_probs (__main__.TestTransformerComponents) ... ok\n",
      "test_transformer_forward_shape (__main__.TestTransformerComponents) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.035s\n",
      "\n",
      "OK\n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.035s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestTransformerComponents(unittest.TestCase):\n",
    "    def test_scaled_dot_product_attention_shape_and_probs(self):\n",
    "        batch, seq_q, seq_k, depth = 2, 3, 4, 8\n",
    "        Q = np.random.randn(batch, seq_q, depth)\n",
    "        K = np.random.randn(batch, seq_k, depth)\n",
    "        V = np.random.randn(batch, seq_k, depth)\n",
    "        out, attn = scaled_dot_product_attention(Q, K, V)\n",
    "        self.assertEqual(out.shape, (batch, seq_q, depth))\n",
    "        self.assertEqual(attn.shape, (batch, seq_q, seq_k))\n",
    "        # attention weights should sum to 1 across keys (allow small numerical error)\n",
    "        s = np.sum(attn, axis=-1)\n",
    "        self.assertTrue(np.allclose(s, 1.0, atol=1e-5))\n",
    "\n",
    "    def test_positional_encoding(self):\n",
    "        pe = positional_encoding(10, 16)\n",
    "        self.assertEqual(pe.shape, (10, 16))\n",
    "        # positions should differ\n",
    "        self.assertFalse(np.allclose(pe[0], pe[1]))\n",
    "\n",
    "    def test_multi_head_attention_shapes(self):\n",
    "        batch, seq, d_model, heads = 2, 5, 16, 4\n",
    "        mha = MultiHeadAttention(d_model, heads)\n",
    "        x = np.random.randn(batch, seq, d_model)\n",
    "        out, attn = mha(x, x, x)\n",
    "        self.assertEqual(out.shape, (batch, seq, d_model))\n",
    "        self.assertEqual(attn.shape, (batch, heads, seq, seq))\n",
    "\n",
    "    def test_ffn_shape(self):\n",
    "        batch, seq, d_model, d_ff = 2, 6, 16, 32\n",
    "        ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "        x = np.random.randn(batch, seq, d_model)\n",
    "        y = ffn(x)\n",
    "        self.assertEqual(y.shape, (batch, seq, d_model))\n",
    "\n",
    "    def test_encoder_decoder_forward_shapes(self):\n",
    "        batch, seq_inp, seq_tar, d_model = 2, 7, 5, 16\n",
    "        enc = Encoder(num_layers=2, d_model=d_model, num_heads=4, d_ff=64)\n",
    "        dec = Decoder(num_layers=2, d_model=d_model, num_heads=4, d_ff=64)\n",
    "        inp = np.random.randn(batch, seq_inp, d_model)\n",
    "        tar = np.random.randn(batch, seq_tar, d_model)\n",
    "        enc_out = enc(inp)\n",
    "        dec_out = dec(tar, enc_out)\n",
    "        self.assertEqual(enc_out.shape, (batch, seq_inp, d_model))\n",
    "        self.assertEqual(dec_out.shape, (batch, seq_tar, d_model))\n",
    "\n",
    "    def test_transformer_forward_shape(self):\n",
    "        batch, seq_inp, seq_tar, d_model = 2, 6, 4, 16\n",
    "        model = Transformer(num_encoder_layers=2, num_decoder_layers=2, d_model=d_model, num_heads=4, d_ff=64)\n",
    "        inp = np.random.randn(batch, seq_inp, d_model)\n",
    "        tar = np.random.randn(batch, seq_tar, d_model)\n",
    "        out = model(inp, tar)\n",
    "        self.assertEqual(out.shape, (batch, seq_tar, d_model))\n",
    "\n",
    "# Run tests and print results\n",
    "suite = unittest.defaultTestLoader.loadTestsFromTestCase(TestTransformerComponents)\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "result = runner.run(suite)\n",
    "if not result.wasSuccessful():\n",
    "    raise SystemExit('Some tests failed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
